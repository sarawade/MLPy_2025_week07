{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "2226bfd2b05b4b2490fe20b9df1e5c12",
    "deepnote_cell_type": "markdown",
    "id": "6Og4DnJPrB4A"
   },
   "source": [
    "# Week 7 - Support Vector Machines\n",
    "\n",
    "### Aims\n",
    "\n",
    "The main concepts covered in this notebook are: \n",
    "\n",
    ">* understanding separable vs non-separable data\n",
    ">* implementing SVMs\n",
    ">* use of different kernels and parameter tuning in SVMs\n",
    "\n",
    "\n",
    "1. [Setup](#setup)\n",
    "\n",
    "2. [SVC](#SVC)\n",
    "\n",
    "3. [Model assessment](#assess)\n",
    "\n",
    "4. [Default Data](#default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6b802cdefbaa481ab91a17945c88fa27",
    "deepnote_cell_type": "markdown",
    "id": "AdHUSbWsvZ7h"
   },
   "source": [
    "This week, we will be exploring the basics of support vector machine models. We will be focusing on support vector machines for classification, which is provided by sklearn in the `SVC` model. For more details, please see https://scikit-learn.org/stable/modules/svm.html\n",
    "\n",
    "The main class that we are using is [sklearn.svm.SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)\n",
    "\n",
    "As usual, during workshops, you will complete the worksheets together in teams of 2-3, using **pair programming**. When completing worksheets:\n",
    "\n",
    ">- You will have tasks tagged by (CORE) and (EXTRA). \n",
    ">- Your primary aim is to complete the (CORE) components during the WS session, afterwards you can try to complete the (EXTRA) tasks for your self-learning process. \n",
    "\n",
    "Instructions for submitting your workshops can be found at the end of worksheet. As a reminder, you must submit a pdf of your notebook on Learn by 16:00 PM on the Friday of the week the workshop was given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "572417335c5941599d7b353361ed867b",
    "deepnote_cell_type": "markdown",
    "id": "6sVlUI4SvZ7i"
   },
   "source": [
    "# Setup <a id='setup'></a>\n",
    "\n",
    "## Packages\n",
    "\n",
    "Let's load the some of the packages needed for this workshop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "c89dcf442b634a7e8eb6cfd395269baa",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2989,
    "execution_start": 1709630355701,
    "id": "grVNp8GrrH0g",
    "output_cleared": true,
    "source_hash": "a458de92"
   },
   "outputs": [],
   "source": [
    "# Data libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# sklearn modules\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC         # SVM\n",
    "from sklearn.preprocessing import StandardScaler # scaling features\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV, KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "This helper function plots the data and visualized the decision boundary and margin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "import sklearn.preprocessing\n",
    "\n",
    "# Visualize the decision boundary and margin\n",
    "# For D=2 inputs and binary classification\n",
    "def plot_margin(model, X, y, figsize=(8,7)):\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1,figsize=figsize)\n",
    "\n",
    "    # Scatter plot of the inputs colored by class \n",
    "    ax.scatter(X[:,0], X[:,1], c=y, s=30)\n",
    "    \n",
    "    # Show decsision boundary\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        model,\n",
    "        X,\n",
    "        plot_method=\"contour\",\n",
    "        colors=\"k\",\n",
    "        levels=[-1, 0, 1],\n",
    "        linestyles=[\"--\", \"-\", \"--\"],\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    # Highlight support vectors\n",
    "    # If pipeline with StandardScalar, inverse transform the support vectors\n",
    "    if (isinstance(model, sklearn.pipeline.Pipeline)):\n",
    "        if (isinstance(model[0], sklearn.preprocessing.StandardScaler)):\n",
    "            support_vectors = model[0].inverse_transform(model[-1].support_vectors_)\n",
    "        else:\n",
    "            support_vectors = model[-1].support_vectors_\n",
    "    else:\n",
    "        support_vectors = model.support_vectors_\n",
    "    ax.scatter(\n",
    "        support_vectors[:, 0],\n",
    "        support_vectors[:, 1],\n",
    "        s=100,\n",
    "        linewidth=1,\n",
    "        facecolors=\"none\",\n",
    "        edgecolors=\"k\",\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d4b4e231b9104da5b5e80f693a4d0d22",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Support Vector Classifier <a id='SVC'></a>\n",
    "\n",
    "The class [`SVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) implements support vector classifiers and support vector machines. When creating an `SVC` object, various options are available, including:\n",
    "\n",
    "- `C`: the inverse regularizaton parameter. NOTE: this defaults to `C=1` but should always be tuned. \n",
    "- `kernel`: options include the  **linear** kernel (`linear`), **polynomial** kernel (`poly`), **radial basis function** kernel (`rbf`), **sigmoid** kernel (`sigmoid`), or a user-defined kernel.\n",
    "- `degree`: degree if using the **polynomial** kernel\n",
    "- `gamma`: kernel coefficient for **rbf**, **polynomial**, or **sigmoid** kernels.\n",
    "- `coef0`: additional coefficient term for the  **polynomial** or **sigmoid** kernels.\n",
    "\n",
    "After calling `.fit()`, the `SVC` object will have a number of attributes including:\n",
    "\n",
    "- `support_vectors_`: containing the support vectors.\n",
    "\n",
    "To predict the class labels from the fitted SVC model, we can call `.predict()`. And although SVMs only provide class labels and not the corresponding class probabilities, `SVC` provides a method to estimate the class probablities by calling `.predict_proba()` (or `.predict_log_proba()` on the log scale) using Platt scaling. This uses cross-validation and makes the implementation slower, thus to turn on this option of estimating the class probabilities, you must first set `probability=True` when creating the `SVC` object. Also note that these probability estimates are unreliable on small datasets.\n",
    "\n",
    "More details on **kernels** are available here: \n",
    "https://scikit-learn.org/stable/modules/svm.html#svm-kernels\n",
    "\n",
    "### **Difference between SVC and LinearSVC**\n",
    "\n",
    "Note the `sklearn` implements two linear support vector classification models `LinearSVC()` and `SVC(kernel='linear')`, which yield slightly different decision boundaries, due to the following differences:\n",
    "\n",
    ">- `LinearSVC` (based on LIBLINEAR) is **faster** than `SVC` (based on LIBSVM)\n",
    ">- `LinearSVC` minimizes the squared hinge loss while SVC minimizes the regular hinge loss.\n",
    ">- `LinearSVC` uses the One-vs-Rest scheme for multiclass classification while `SVC` uses the One-vs-One scheme for multiclass classification.\n",
    ">- `LinearSVC` does not provide some of the attributes of `SVC`, such as the support vectors.\n",
    "\n",
    "For further details, see the documentations of the two classes\n",
    "\n",
    "- `SVC` with `kernel=linear`: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "- `LinearSVC`: https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
    "\n",
    "In the following, we will focus on the `SVC` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8ae8d64d1f0a479b9e8cb56fbf3342bc",
    "deepnote_cell_type": "markdown",
    "id": "yz3bjxcbvZ7r"
   },
   "source": [
    "## Linearly Separable Data \n",
    "\n",
    "We will begin by examining various toy data sets to explore the basics of these models. For the first example, we will read in data from `ex1.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "2b0a51acf8104ec38b06da121427718d",
    "deepnote_cell_type": "code",
    "deepnote_table_loading": false,
    "deepnote_table_state": {
     "filters": [],
     "pageIndex": 0,
     "pageSize": 25,
     "sortBy": []
    },
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 151,
    "execution_start": 1709630373982,
    "id": "eF19U6ivvZ7r",
    "source_hash": "c09f28e9"
   },
   "outputs": [],
   "source": [
    "ex1 = pd.read_csv(\"ex1.csv\")\n",
    "ex1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9831bee99c4a475daad1b00555db7157",
    "deepnote_cell_type": "markdown",
    "id": "izo4A3SSvZ7t"
   },
   "source": [
    "Plotting the data below, we can see the that data is composed of two classes in two dimensions, and it is clear that these two classes are linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "f374ae6831ef4a58b4c61f88c77032d8",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 548
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 364,
    "execution_start": 1709630377917,
    "id": "r3QIcTBsCZAf",
    "outputId": "5c544249-18e4-4129-be03-fc2bc14f45cb",
    "source_hash": "c77bc8e2"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,6))\n",
    "sns.scatterplot(x='x1', y='x2', hue='y', data=ex1, legend=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let separate the features and outcome and encode the outcome to a binary vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the features and output and encoding y\n",
    "X_ex1 = np.array(ex1.drop('y', axis=1))\n",
    "y_ex1 = LabelEncoder().fit_transform(ex1.y)\n",
    "\n",
    "print(X_ex1.shape)\n",
    "print(y_ex1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7298ca71ec0748f6b2c540d423562be9",
    "deepnote_cell_type": "markdown",
    "id": "50AbKP76vZ7u"
   },
   "source": [
    "### 🚩 Exercise 1 (CORE)\n",
    "\n",
    "a. Create and fit an SVC model using a **linear kernel** and `C=100`.\n",
    "\n",
    "b. Visualize the decision boundary, margin, and support vectors using the function `plot_margin` defined above. How many support vectors are there for each class? Are they on right side of the margin? hyperplane?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "04b3c5ce6d4e4c939294d32983f06cff",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 207,
    "execution_start": 1709630390466,
    "source_hash": "b9b0acba"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "debdc28c36724f239e946e4e8cd526bf",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### 🚩 Exercise 2  (CORE)\n",
    "\n",
    "Now, let's see how the results change with a small value of `C`.\n",
    "\n",
    "a. Create and fit an SVC model using a **linear kernel** and `C=1`.\n",
    "\n",
    "b. Visualize the decision boundary, margin, and support vectors. Now, how many support vectors are there for each class? Are they on right side of the margin? hyperplane? How has the margin changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "784b7868bfe3464ca7ae9aea63256572",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 449,
    "execution_start": 1709630414273,
    "source_hash": "d9f6c6cf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b1aa5e417376414285357380f0f1fd92",
    "deepnote_cell_type": "markdown",
    "id": "AKglNm6mD_jx"
   },
   "source": [
    "## Non-Separable Data\n",
    "\n",
    "Next, we complicate our previous example by adding two additional points from class A to our data, which result in data that are no longer linearly separable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "c421c5e5ee3942238dcacfd7892cbd3e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 548
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 284,
    "execution_start": 1709630419442,
    "id": "7XEqi56tEHsB",
    "outputId": "d9414ca1-3126-4d20-ad0f-dd5602238d03",
    "source_hash": "96a0045d"
   },
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "ex2 = pd.read_csv(\"ex2.csv\")\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(7,6))\n",
    "sns.scatterplot(x='x1', y='x2', hue='y', data=ex2, legend=False)\n",
    "plt.show()\n",
    "\n",
    "# Extracting the features and output and encoding y\n",
    "X_ex2 = np.array(ex2.drop('y', axis=1))\n",
    "y_ex2 = LabelEncoder().fit_transform(ex2.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ed3f184c570f4b88ac81ec9840c1c5de",
    "deepnote_cell_type": "markdown",
    "id": "O6TMeCFgAJVt"
   },
   "source": [
    "### 🚩 Exercise 3  (CORE)\n",
    "\n",
    "Fit the SVC model using the **linear kernel** and **same choices of `C=100` and `C=1`**. How have the results changed? Comment on changes related to the margin and the number of support vectors for each class and their location relative to the margin and hyperplane. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "ef641bf6c729427fb2bd958ca5b45d3a",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 90,
    "execution_start": 1709630434274,
    "source_hash": "f1a84fa7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a5f46084cf294ed7bc9052572d3ad95b",
    "deepnote_cell_type": "markdown"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "083eb8155f5e436aaf2d2d8c10075676",
    "deepnote_cell_type": "markdown",
    "id": "_YZt5su8C7WZ"
   },
   "source": [
    "## Nonlinear Separation\n",
    "\n",
    "Next, we will look at a new data set that is not linearly separable, but can be perfectly separated by a nonlinear decision boundary. We will start by loading and visualizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "2d582e5549de411ca4e006aa7bb61465",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 548
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 397,
    "execution_start": 1709630443674,
    "id": "jSwzAMONFJm1",
    "outputId": "229a9a69-2e51-4ff8-b902-ac52fc25c439",
    "source_hash": "1e13e984"
   },
   "outputs": [],
   "source": [
    "# Load the third data set\n",
    "ex3 = pd.read_csv(\"ex3.csv\")\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(7,6))\n",
    "sns.scatterplot(x='x1', y='x2', hue='y', data=ex3, legend=False)\n",
    "plt.show()\n",
    "\n",
    "# Extracting the features and output and encoding y\n",
    "X_ex3 = np.array(ex3.drop('y', axis=1))\n",
    "y_ex3 = LabelEncoder().fit_transform(ex3.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by trying to fit a linear SVC. As expected, the results are terrible and the number of support vectors is very large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the SVC with a linear kernel on the new data \n",
    "svm_lin = SVC(kernel='linear', C=100).fit(X_ex3, y_ex3)\n",
    "\n",
    "# Visualize the decision boundary, margin and support vectors\n",
    "plot_margin(svm_lin, X_ex3,y_ex3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6cc5a5bac98e43bf98b01a0149cde26b",
    "deepnote_cell_type": "markdown",
    "id": "mvrOq4afvZ7z"
   },
   "source": [
    "### 🚩 Exercise 4  (CORE)\n",
    "\n",
    "- Fit an SVM with a **polynomial kernel with degree 2** and `C=100`.\n",
    "- Visualize the margin and decision boundary and comment on the number of support vectors compared to the linear model.\n",
    "- Compute and visualize the confusion matrix using `ConfusionMatrixDisplay.from_estimator`. Is the fitted SVM able to perfectly separate the classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "77005aa474f0457da708047f84a9f208",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 369,
    "execution_start": 1709630455632,
    "source_hash": "805fed8e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c5e671a1550b4ec483e7155a334ac3d0",
    "deepnote_cell_type": "markdown"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "1880656f8a2744d4854cbe68bad1e120",
    "deepnote_cell_type": "markdown",
    "id": "TLaXXgVqGQrd"
   },
   "source": [
    "## Kernels for SVMs and Parameter-Tuning\n",
    "\n",
    "Next, we will consider an more complicated data, and explore using grid search to tune the model parameters and the effect of different kernels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "21d166de120040cea2985498a930aedd",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 548
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 446,
    "execution_start": 1709630463309,
    "id": "fAZUQs23GXAl",
    "outputId": "5634934d-e3f0-4972-8e54-d5b12da535d3",
    "source_hash": "815fe764"
   },
   "outputs": [],
   "source": [
    "# Load the fourth data set\n",
    "ex4 = pd.read_csv(\"ex4.csv\")\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(7,6))\n",
    "sns.scatterplot(x='x1', y='x2', hue='y', data=ex4, legend=False)\n",
    "plt.show()\n",
    "\n",
    "# Extracting the features and output and encoding y\n",
    "X_ex4 = np.array(ex4.drop('y', axis=1))\n",
    "y_ex4 = LabelEncoder().fit_transform(ex4.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the polynomial kernel that we saw in the previous exercise and use cross-validation to tune both the degree of polynomial and the penalty parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with polynomial kernel\n",
    "svm = SVC(kernel='poly', coef0=1)\n",
    "\n",
    "# Grid search over C and the degree of the polynomial\n",
    "degrees = [1,2,3,4]\n",
    "C = np.linspace(0.1, 10, 100)\n",
    "cv = GridSearchCV(\n",
    "    svm,\n",
    "    param_grid = {'C': C,\n",
    "        'degree': degrees},\n",
    "    cv = KFold(5, shuffle = True, random_state = 0)\n",
    ")\n",
    "\n",
    "# Fit and tune the model\n",
    "cv.fit(X_ex4, y_ex4)\n",
    "\n",
    "# Get the best model parameters and the accuracy of the model\n",
    "print(\"Params: \", cv.best_params_)\n",
    "print(\"Avg Accuracy: \", cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚩 Exercise 5  (CORE)\n",
    "\n",
    "- Run the following code to plot the CV accuracy. Based on this plot, would you use the best parameter values printed above or choose different values? Why?\n",
    "- For your selected parameters, fit the svm and plot the decision boundary and margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store cv scores in a data frame \n",
    "cv_accuracy = pd.DataFrame(cv.cv_results_\n",
    "                           ).filter(['param_C', 'param_degree','mean_test_score']\n",
    "                                    ).rename(columns={'param_C':'C', 'param_degree':'degree','mean_test_score':'CV_score'})\n",
    "\n",
    "# Plot the CV scores\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.lineplot(x='C', y='CV_score', data = cv_accuracy, hue ='degree', palette=\"Spectral\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚩 Exercise 6  (EXTRA)\n",
    "\n",
    "- Repeat the grid search above with a polynomial kernel but set the coefficent term `coef0=0` (this is the default value). Redraw the plot of the CV accuracy. How have the results changed? Can you explain why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f5162008121b4fae8b6603c9152aeda6",
    "deepnote_cell_type": "markdown",
    "id": "z4NLHMUEdKxP"
   },
   "source": [
    "### 🚩 Exercise 7  (CORE)\n",
    "\n",
    "Consider instead the RBF kernel. \n",
    "\n",
    "- Use grid search to tune both the penalty parameter $C$ and the inverse bandwidth parameter $\\gamma$.\n",
    "\n",
    "- Plot the decision boundary. How do the results compare to the SVM with a polynomial kernel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "bfad448c9fab49928ff9ae77ab1fd035",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 47,
    "execution_start": 1709630469323,
    "id": "9vTpIe932BFz",
    "source_hash": "9010ccb5"
   },
   "outputs": [],
   "source": [
    "# Values of C and the inverse bandwidth\n",
    "gamma = [1/4,1/2,1,2,4]\n",
    "C = np.linspace(0.1, 10, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ead67ceb235e4c8496c153a29d44c08b",
    "deepnote_cell_type": "markdown"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "02909f2981344600aebedc8a1b07a959",
    "deepnote_cell_type": "markdown",
    "id": "2uoEYNyw0Jfm"
   },
   "source": [
    "##  Model Assessment <a id='asses'></a>\n",
    "\n",
    "As, we learned last week, there are many metrics to consider beyond accuracy. For example, below we compute and print the classification report, summarizing the results for precision, recall, f1-score, and accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Classification report for the SVM with polynomial kernel\n",
    "print(classification_report(y_ex4, cv.best_estimator_.predict(X_ex4)))\n",
    "\n",
    "# Classification report for the SVM with RBF kernel\n",
    "print(classification_report(y_ex4, cv2.best_estimator_.predict(X_ex4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We also learned about other tools, such as the ROC curve, AUC, and precision-recall curve. However, since the SVMs are not model-based, **we only obtain hard label assignments when doing predictions**. To overcome this, one heuristic that can be used is **Platt scaling** to convert the SVM output to probabilities. However, these probabilites may not be well calibrated and may be inconsistent with the hard labels.\n",
    "\n",
    "In the code below, we visualize the probabilities of class assignments across a grid of possible inputs for the tuned SVM polynomial model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the polynomial model with optimal prarmeters and the option to compute the probabilities\n",
    "svm_poly = SVC(kernel='poly', C=cv.best_params_[\"C\"], degree=cv.best_params_[\"degree\"], probability=True).fit(X_ex4, y_ex4)\n",
    "\n",
    "# Create a grid of inputs for plotting\n",
    "x1lim = [X_ex4[:,0].min(),X_ex4[:,0].max()]\n",
    "x2lim = [X_ex4[:,1].min(),X_ex4[:,1].max()]\n",
    "    \n",
    "xx1 = np.linspace(x1lim[0]-1, x1lim[1]+1, 50)\n",
    "xx2 = np.linspace(x2lim[0]-1, x2lim[1]+1, 50)\n",
    "XX2, XX1 = np.meshgrid(xx2, xx1)\n",
    "    \n",
    "# Calculate the probabiltiy for each point in the grid\n",
    "xx = np.c_[XX1.ravel(), XX2.ravel()]\n",
    "xx.shape\n",
    "P = svm_poly.predict_proba(xx)[:,0].reshape(XX1.shape)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.contourf(XX1, XX2, P)\n",
    "plt.colorbar()\n",
    "plt.scatter(x=X_ex4[:,0], y=X_ex4[:,1],c = 1-y_ex4, marker = '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the model is quite **confident in the predictions (probabilities close to 0 or 1)** in the corners of the input space, even where we don't have any data. In some cases, this may be undesirable, as we may not want to make such confident assesments in areas where we have little to no data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚩 Exercise 8  (EXTRA)\n",
    "\n",
    "- For the SVM with RBF kernel and optimal CV parameters, repeat the plot above to visualize the probabilities of class assignments.\n",
    "- How do the results compare with the polynomial kernel? Does this impact your choice of kernel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "80aac224d6544d6e89dd6b0125ec43d6",
    "deepnote_cell_type": "markdown",
    "id": "YwCfjeYB4tVM"
   },
   "source": [
    "# Default Data <a id='default'></a>\n",
    "\n",
    "Let's consider the default data that we explored last week. Recall that information is collected on **10000** individuals, recording whether they defaulted on their credit card or not as well as other characteristics. Specifically, the included columns in the data are:\n",
    "\n",
    "* `default` - whether the individual has defaulted\n",
    "\n",
    "* `student` - whether the individual is the student\n",
    "\n",
    "* `balance` - balance in the individual's account\n",
    "\n",
    "* `income` - income of the individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "662e54cd8b554bf6877c701526d7c19d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 237,
    "execution_start": 1709630556944,
    "id": "SlbaLT675J8g",
    "outputId": "fa3accef-409c-41d9-b61d-2ac6b5d8f4b6",
    "source_hash": "eb028808"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_default = pd.read_csv(\"Default.csv\", index_col=0)\n",
    "\n",
    "# For ease of exposition, let's' drop the student varible.\n",
    "df_default = df_default.drop(\"student\", axis=1)\n",
    "df_default.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our feature matrix and output vector, and split the data into a train and test set. Recall that due to the class imbalance in the data, we use the option `stratify=y` to maintain the class proportion in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Feature matrix and response vector\n",
    "X, y = df_default.drop(['default'], axis=1), df_default['default']\n",
    "\n",
    "# Convert to numpy\n",
    "X = X.values\n",
    "\n",
    "# Encode default\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "# Stratify split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle= True, stratify=y,\n",
    "                                                    test_size = 0.1, random_state=1112)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "2c22f89a319749eebef19677b97d2cde",
    "deepnote_cell_type": "markdown",
    "id": "MAqFPdjr50B-"
   },
   "source": [
    "### 🚩 Exercise 9 (CORE)\n",
    "\n",
    "a. Run the following code to fit an SVC and tune the penalty parameter `C`.\n",
    "\n",
    "b. Plot the accuracy, recall, and f1 score as function of `C` and visualize the decision boundary. Comment on the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM with linear kernel\n",
    "svm_lin = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SVC(kernel='linear')\n",
    ")\n",
    "\n",
    "# Grid search over C \n",
    "C = np.linspace(0.1, 10, 10)\n",
    "cv_lin = GridSearchCV(\n",
    "    svm_lin,\n",
    "    param_grid = {'svc__C': C},\n",
    "    cv = KFold(5, shuffle = True, random_state = 0),\n",
    "    scoring = [\"accuracy\", \"f1\",\"recall\"],\n",
    "    refit='recall' #refit based on recall\n",
    ")\n",
    "\n",
    "# Fit and tune the model\n",
    "cv_lin.fit(X_train, y_train)\n",
    "\n",
    "# Store cv scores in a data frame \n",
    "cv_accuracy = pd.DataFrame(cv_lin.cv_results_\n",
    "                           ).filter(['param_svc__C','mean_test_accuracy','mean_test_f1', 'mean_test_recall']\n",
    "                                    ).rename(columns={'param_svc__C':'C','mean_test_accuracy':'CV accuracy', 'mean_test_f1':'CV f1', 'mean_test_recall':'CV recall'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "76565b6069414cbb9c83473747ba8d0f",
    "deepnote_cell_type": "markdown"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "03832372c7084cdaaccc684357d63ded",
    "deepnote_cell_type": "markdown",
    "id": "WdZKHECXTmgU"
   },
   "source": [
    "### 🚩 Exercise 10 (CORE)\n",
    "\n",
    "a. To address the imbalance issue, alter the pipeline either using the `RandomOverSampler` or `RandomUnderSampler`.\n",
    "\n",
    "b. Choose a value of `C` and plot the decision boundary and confusion matrix on the test data. How have the results changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the imblearn if necessary \n",
    "# !pip install imblearn\n",
    "\n",
    "from imblearn.pipeline import make_pipeline as Im_make_pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚩 Exercise 11 (CORE)\n",
    "\n",
    "Lastly, let's explore using a nonlinear decision boundary. Alter your pipeline from the previous exercise to use a polynomial kernel of degree 2 with a coefficient of 1. Visualize the decision boundary and confusion matrix on the test data. How do the results compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "05dbb2fa58a34cc0a5960134c4ce6ffa",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Competing the Worksheet\n",
    "\n",
    "At this point you have hopefully been able to complete all the CORE exercises and attempted the EXTRA ones. Now \n",
    "is a good time to check the reproducibility of this document by restarting the notebook's\n",
    "kernel and rerunning all cells in order.\n",
    "\n",
    "Before generating the PDF, please go to Edit -> Edit Notebook Metadata and change 'Student 1' and 'Student 2' in the **name** attribute to include your name. If you are unable to edit the Notebook Metadata, please add a Markdown cell at the top of the notebook with your name(s).\n",
    "\n",
    "Once that is done and you are happy with everything, you can then run the following cell \n",
    "to generate your PDF. Once generated, please submit this PDF on Learn page by 16:00 PM on the Friday of the week the workshop was given. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "e2f940456bb2428bb2f4ac63805b21b2",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to pdf mlp_week07.ipynb "
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Student 1"
   },
   {
    "name": "Student 2"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "title": "MLPy Workshop 7"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
